#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
================================================================================
Pipeline è¼¸å‡ºè½‰è¨“ç·´è³‡æ–™å·¥å…·
================================================================================

ã€ç”¨é€”ã€‘
å°‡ run_bom_pipelineV2.py è¼¸å‡ºçš„ REVIEW æª”æ¡ˆï¼Œ
ç¶“å» å•†äººå·¥å¯©æ ¸ä¿®æ­£å¾Œï¼Œè½‰æ›ç‚º NER æ¨¡å‹è¨“ç·´æ ¼å¼ã€‚

ã€å·¥ä½œæµç¨‹ã€‘
1. Pipeline è™•ç† â†’ ç”¢å‡º _REVIEW.xlsx
2. å» å•†äººå·¥å¯©æ ¸ â†’ ä¿®æ­£æ¬„ä½å€¼
3. æœ¬å·¥å…·è½‰æ› â†’ ç”¢å‡ºè¨“ç·´æ ¼å¼çš„ Excel
4. è¨“ç·´è…³æœ¬é‡æ–°è¨“ç·´ â†’ æ¨¡å‹æ•ˆèƒ½æå‡

ã€ä½¿ç”¨æ–¹å¼ã€‘

    python convert_to_training_data.py --input "å¯©æ ¸å®Œæˆ.xlsx" --output "æ–°è¨“ç·´è³‡æ–™.xlsx"

================================================================================
"""
from __future__ import annotations

import argparse
import re
from pathlib import Path
from typing import List, Tuple

import pandas as pd


# =============================================================================
# æ¬„ä½åç¨±å°ç…§ï¼ˆä¸­æ–‡ â†’ NER æ¨™ç±¤ï¼‰
# =============================================================================
FIELD_TO_LABEL = {
    "é¡åˆ¥": "Category",
    "é˜»å€¼": "Resistance",
    "é˜»å€¼_IEC": None,  # è¡ç”Ÿæ¬„ä½ï¼Œä¸ç´å…¥è¨“ç·´
    "å®¹é‡": "Capacitance",
    "å®¹é‡_EIA": None,  # è¡ç”Ÿæ¬„ä½ï¼Œä¸ç´å…¥è¨“ç·´
    "é›»æ„Ÿå€¼": "Inductance",
    "é›»å£“": "Voltage",
    "é›»æµ": "Current",
    "å®¹å·®": "Tolerance",
    "åŠŸç‡": "Power",
    "æº«åº¦ä¿‚æ•¸": "Temp_Coefficient",
    "ä»‹è³ª": "Temp_Code",
    "é¡è‰²": "Color",
    "é »ç‡": "Frequency",
    "æ³¢é•·": "Wavelength",
    "é–“è·": "Size",  # é–“è·é€šå¸¸èˆ‡å°ºå¯¸åˆä½µ
    "å°ºå¯¸": "Size",
    "å°è£": "Package",
    "é‡è…³æ•¸": "Pin_Count",
    "æ–¹å‘": "Type",
    "é¡å‹": "Type",
    "æ³•è¦": "Compliance",
    "è£½ç¨‹": "Process_Type",
}


def simple_tokenize(text: str) -> List[str]:
    """
    ç°¡æ˜“åˆ†è©å™¨ï¼ˆèˆ‡è¨“ç·´è…³æœ¬ä¸€è‡´ï¼‰
    """
    if not isinstance(text, str):
        return []
    tokens = []
    for m in re.finditer(r'\S+', text):
        segment = m.group()
        for sub in re.finditer(r'[A-Za-z0-9.+/%Î©Âµ]+|[^A-Za-z0-9.+/%Î©Âµ]', segment):
            tokens.append(sub.group())
    return tokens


def build_labels_from_fields(tokens: List[str], row: pd.Series) -> List[str]:
    """
    æ ¹æ“š tokens å’Œæ¬„ä½å€¼å»ºç«‹ labels
    
    ç­–ç•¥ï¼š
    1. å°æ¯å€‹ tokenï¼Œæª¢æŸ¥å®ƒæ˜¯å¦å‡ºç¾åœ¨ä»»ä½•å·²çŸ¥æ¬„ä½å€¼ä¸­
    2. è‹¥æ‰¾åˆ°åŒ¹é…ï¼Œè³¦äºˆå°æ‡‰çš„ NER æ¨™ç±¤
    3. è‹¥æœªæ‰¾åˆ°ï¼Œæ¨™è¨˜ç‚º "O"ï¼ˆå…¶ä»–ï¼‰æˆ– "IGNORE"ï¼ˆç¬¦è™Ÿï¼‰
    """
    labels = []
    
    # å»ºç«‹æ¬„ä½å€¼ â†’ æ¨™ç±¤çš„æ˜ å°„
    value_to_label = {}
    for field_name, ner_label in FIELD_TO_LABEL.items():
        if ner_label is None:
            continue
        value = str(row.get(field_name, "")).strip()
        if value:
            # å°‡æ¬„ä½å€¼åˆ†è©å¾Œï¼Œæ¯å€‹ token éƒ½å°æ‡‰åˆ°è©²æ¨™ç±¤
            field_tokens = simple_tokenize(value)
            for ft in field_tokens:
                if ft not in value_to_label:
                    value_to_label[ft.upper()] = ner_label
    
    # å°æ¯å€‹ token å°‹æ‰¾åŒ¹é…
    for tok in tokens:
        tok_upper = tok.upper()
        
        # æª¢æŸ¥æ˜¯å¦ç‚ºç¬¦è™Ÿ
        if re.match(r'^[,;:\(\)\[\]\/\-\+\*\&\|\!\?\.\s]+$', tok):
            labels.append("IGNORE")
        # æª¢æŸ¥æ˜¯å¦åœ¨å·²çŸ¥æ¬„ä½å€¼ä¸­
        elif tok_upper in value_to_label:
            labels.append(value_to_label[tok_upper])
        # æª¢æŸ¥éƒ¨åˆ†åŒ¹é…ï¼ˆä¾‹å¦‚ "10K" åŒ…å«åœ¨ "10KÎ©"ï¼‰
        else:
            found = False
            for val, lbl in value_to_label.items():
                if tok_upper in val or val in tok_upper:
                    labels.append(lbl)
                    found = True
                    break
            if not found:
                labels.append("O")
    
    return labels


def convert_row(row: pd.Series, desc_col: str = "æ­£è¦åŒ–Description") -> Tuple[str, str]:
    """
    å°‡ä¸€ç­†è³‡æ–™è½‰æ›ç‚ºè¨“ç·´æ ¼å¼
    
    Returns:
        (description_raw, labels_str)
    """
    # å–å¾—åŸå§‹æè¿°
    desc = str(row.get(desc_col, "")).strip()
    if not desc:
        desc = str(row.get("description_raw", "")).strip()
    
    # åˆ†è©
    tokens = simple_tokenize(desc)
    
    # å»ºç«‹æ¨™ç±¤
    labels = build_labels_from_fields(tokens, row)
    
    # ç¢ºä¿é•·åº¦ä¸€è‡´
    if len(tokens) != len(labels):
        # ä¿®æ­£é•·åº¦ä¸ä¸€è‡´
        labels = labels[:len(tokens)] + ["O"] * (len(tokens) - len(labels))
    
    return desc, str(labels)


def convert_excel(input_path: Path, output_path: Path, desc_col: str = "æ­£è¦åŒ–Description"):
    """
    è½‰æ›æ•´å€‹ Excel æª”æ¡ˆ
    """
    print(f"ğŸ“¥ è®€å–æª”æ¡ˆï¼š{input_path}")
    df = pd.read_excel(input_path)
    print(f"   å…± {len(df)} ç­†è³‡æ–™")
    
    results = []
    skipped = 0
    
    for idx, row in df.iterrows():
        try:
            desc, labels_str = convert_row(row, desc_col)
            if desc and labels_str != "[]":
                results.append({
                    "Description": desc,
                    "Labels": labels_str,
                })
            else:
                skipped += 1
        except Exception as e:
            print(f"   âš ï¸ ç¬¬ {idx+1} ç­†è·³éï¼š{e}")
            skipped += 1
    
    out_df = pd.DataFrame(results)
    out_df.to_excel(output_path, index=False)
    
    print(f"âœ… å®Œæˆï¼")
    print(f"   è¼¸å‡ºï¼š{output_path}")
    print(f"   æˆåŠŸï¼š{len(results)} ç­†")
    print(f"   è·³éï¼š{skipped} ç­†")


def main():
    parser = argparse.ArgumentParser(
        description="å°‡ Pipeline è¼¸å‡ºè½‰æ›ç‚º NER è¨“ç·´è³‡æ–™æ ¼å¼"
    )
    parser.add_argument(
        "--input", "-i",
        required=True,
        help="è¼¸å…¥çš„ Excel æª”æ¡ˆï¼ˆå» å•†å¯©æ ¸å®Œæˆçš„ _REVIEW.xlsxï¼‰"
    )
    parser.add_argument(
        "--output", "-o",
        default=None,
        help="è¼¸å‡ºçš„ Excel æª”æ¡ˆï¼ˆè¨“ç·´æ ¼å¼ï¼‰ã€‚é è¨­ï¼š<è¼¸å…¥æª”å>_training.xlsx"
    )
    parser.add_argument(
        "--desc_col",
        default="æ­£è¦åŒ–Description",
        help="æè¿°æ¬„ä½åç¨±ã€‚é è¨­ï¼šæ­£è¦åŒ–Description"
    )
    
    args = parser.parse_args()
    
    input_path = Path(args.input).expanduser().resolve()
    if not input_path.exists():
        raise FileNotFoundError(f"æ‰¾ä¸åˆ°è¼¸å…¥æª”æ¡ˆï¼š{input_path}")
    
    if args.output:
        output_path = Path(args.output).expanduser().resolve()
    else:
        output_path = input_path.parent / f"{input_path.stem}_training.xlsx"
    
    convert_excel(input_path, output_path, args.desc_col)


if __name__ == "__main__":
    main()
